{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKFXZVo2D9M/uUGiKNV0xz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruitenbeek/thesis/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQrBqMLM09PN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3b2e43-4b68-46f6-eb99-d74ca0b5f929"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNFvJn-fakmQ",
        "outputId": "d87e7033-3f40-4096-ff6e-3592eb1b22cd"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/thesis/code')\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/thesis/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbQzrgNx4JuT",
        "outputId": "cfb86078-97bf-4342-ca1b-8ed1cd62d44c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MCjtqr3bY4G"
      },
      "source": [
        "from csv import DictReader\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juzqtN23a_CR"
      },
      "source": [
        "def read_file(file):\n",
        "    data = list()\n",
        "    abu_count = 0\n",
        "    off_count = 0\n",
        "    not_count = 0\n",
        "    with open(file, 'r') as f:\n",
        "        reader = DictReader(f, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            if (row['abusive'] == 'NOT' or row['abusive'] == 'UNKNOWN') and (row['explicitness'] == 'IMPLICIT' or row['explicitness'] == 'EXPLICIT'):\n",
        "                data.append([row['text'], 'OFF'])\n",
        "                off_count += 1\n",
        "            elif row['abusive'] == 'IMPLICIT' or row['abusive'] == 'EXPLICIT':\n",
        "                data.append([row['text'], 'ABU'])\n",
        "                abu_count += 1\n",
        "            elif (row['abusive'] == 'NOT' or row['abusive'] == 'UNKNOWN') and row['explicitness'] == 'NOT':\n",
        "                data.append([row['text'], 'NOT'])\n",
        "                not_count += 1\n",
        "    print(f'ABU: %i\\nOFF: %i\\nNOT: %i' % (abu_count, off_count, not_count))\n",
        "    data_df = pd.DataFrame(data)\n",
        "    data_df.columns = ['text', 'label']\n",
        "    return data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInhIpD6zDEr"
      },
      "source": [
        "def read_lex(file):\n",
        "    abu_words = list()\n",
        "    with open(file, 'r') as f:\n",
        "        reader = DictReader(f, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            abu_words.append(row['lemma'])\n",
        "    return set(abu_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKSlfbY1z1aL"
      },
      "source": [
        "def find_lexwords(df, lex):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    abu_lextweets = 0\n",
        "    off_lextweets = 0\n",
        "    not_lextweets = 0\n",
        "    abu_df, not_df, off_df = df.groupby('label')\n",
        "    for text in abu_df[1]['text']:\n",
        "        tokens = word_tokenize(text)\n",
        "        for token in tokens:\n",
        "            if lemmatizer.lemmatize(token) in lex:\n",
        "                abu_lextweets += 1\n",
        "                break\n",
        "    for text in not_df[1]['text']:\n",
        "        tokens = word_tokenize(text)\n",
        "        for token in tokens:\n",
        "            if lemmatizer.lemmatize(token) in lex:\n",
        "                not_lextweets += 1\n",
        "                break\n",
        "    for text in off_df[1]['text']:\n",
        "        tokens = word_tokenize(text)\n",
        "        for token in tokens:\n",
        "            if lemmatizer.lemmatize(token) in lex:\n",
        "                off_lextweets += 1\n",
        "                break\n",
        "    print(f'Lex word Tweets ABU: %i\\nLex word Tweets OFF: %i\\nLex word Tweets NOT: %i\\n' % (abu_lextweets, off_lextweets, not_lextweets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udjZFFe5b8r-",
        "outputId": "48a61af1-cb3b-4257-eb97-b1548d8c974f"
      },
      "source": [
        "train_data = read_file('train_final_pp.csv')\n",
        "dev_data = read_file('dev_final_pp.csv')\n",
        "test_data = read_file('test_final_pp.csv')\n",
        "lex = read_lex('groflex.tsv')\n",
        "\n",
        "print('\\nTrain')\n",
        "find_lexwords(train_data, lex)\n",
        "print('\\nDev')\n",
        "find_lexwords(dev_data, lex)\n",
        "print('\\nTest')\n",
        "find_lexwords(test_data, lex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ABU: 1143\n",
            "OFF: 1445\n",
            "NOT: 5176\n",
            "ABU: 110\n",
            "OFF: 76\n",
            "NOT: 361\n",
            "ABU: 637\n",
            "OFF: 399\n",
            "NOT: 2072\n",
            "\n",
            "Train\n",
            "Lex word Tweets ABU: 341\n",
            "Lex word Tweets OFF: 176\n",
            "Lex word Tweets NOT: 68\n",
            "\n",
            "\n",
            "Dev\n",
            "Lex word Tweets ABU: 37\n",
            "Lex word Tweets OFF: 9\n",
            "Lex word Tweets NOT: 5\n",
            "\n",
            "\n",
            "Test\n",
            "Lex word Tweets ABU: 222\n",
            "Lex word Tweets OFF: 63\n",
            "Lex word Tweets NOT: 28\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}