{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2step_lr.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPHupns0niPLbwUVD+QIKg+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruitenbeek/thesis/blob/main/2step_lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm29QK4v2sQQ",
        "outputId": "3220cc5a-86fe-407b-f33b-71a064a03dbe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrq64v5q26HA",
        "outputId": "cae279a2-8315-4b0d-a023-b7f98628d456"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/thesis/code')\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/thesis/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuX8LPhH3shi"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH3I7icL27o4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from csv import DictReader\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoPztAFi3vsv"
      },
      "source": [
        "#Read Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iJrnR3-3rhc"
      },
      "source": [
        "def read_train_file(file):\n",
        "    data1 = list()\n",
        "    data2 = list()\n",
        "    abu_count = 0\n",
        "    off_count = 0\n",
        "    not_count = 0\n",
        "    with open(file, 'r') as f:\n",
        "        reader = DictReader(f, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            if (row['abusive'] == 'NOT' or row['abusive'] == 'UNKNOWN') and (row['explicitness'] == 'IMPLICIT' or row['explicitness'] == 'EXPLICIT'):\n",
        "                data1.append([row['text'], 'OFF'])\n",
        "                data2.append([row['text'], 'OFF'])\n",
        "                off_count += 1\n",
        "            elif row['abusive'] == 'IMPLICIT' or row['abusive'] == 'EXPLICIT':\n",
        "                data1.append([row['text'], 'OFF'])\n",
        "                data2.append([row['text'], 'ABU'])\n",
        "                abu_count += 1\n",
        "            elif (row['abusive'] == 'NOT' or row['abusive'] == 'UNKNOWN') and row['explicitness'] == 'NOT':\n",
        "                data1.append([row['text'], 'NOT'])\n",
        "                not_count += 1\n",
        "    print(f'ABU: %i\\nOFF: %i\\nNOT: %i' % (abu_count, off_count, not_count))\n",
        "    data1_df = pd.DataFrame(data1)\n",
        "    data1_df.columns = ['text', 'label']\n",
        "    data2_df = pd.DataFrame(data2)\n",
        "    data2_df.columns = ['text', 'label']\n",
        "    return data1_df, data2_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6qSkcAEeIiq"
      },
      "source": [
        "def read_test_file(file):\n",
        "    data1 = list()\n",
        "    data2 = list()\n",
        "    abu_count = 0\n",
        "    off_count = 0\n",
        "    not_count = 0\n",
        "    with open(file, 'r') as f:\n",
        "        reader = DictReader(f, delimiter='\\t')\n",
        "        for row in reader:\n",
        "            if (row['abusive'] == 'NOT' or row['abusive'] == 'UNKNOWN') and (row['explicitness'] == 'IMPLICIT' or row['explicitness'] == 'EXPLICIT'):\n",
        "                data1.append([row['text'], 'OFF'])\n",
        "                data2.append([row['text'], 'OFF'])\n",
        "                off_count += 1\n",
        "            elif row['abusive'] == 'IMPLICIT' or row['abusive'] == 'EXPLICIT':\n",
        "                data1.append([row['text'], 'OFF'])\n",
        "                data2.append([row['text'], 'ABU'])\n",
        "                abu_count += 1\n",
        "            elif (row['abusive'] == 'NOT' or row['abusive'] == 'UNKNOWN') and row['explicitness'] == 'NOT':\n",
        "                data1.append([row['text'], 'NOT'])\n",
        "                data2.append([row['text'], 'NOT'])\n",
        "                not_count += 1\n",
        "    print(f'ABU: %i\\nOFF: %i\\nNOT: %i' % (abu_count, off_count, not_count))\n",
        "    data1_df = pd.DataFrame(data1)\n",
        "    data1_df.columns = ['text', 'label']\n",
        "    data2_df = pd.DataFrame(data2)\n",
        "    data2_df.columns = ['text', 'label']\n",
        "    return data1_df, data2_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvZ9MnkPo8gr"
      },
      "source": [
        "#Split Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN1_-Ngjo6ey"
      },
      "source": [
        "def split_labels(data_df):\n",
        "    data_X = data_df.text.tolist()\n",
        "    data_y = data_df.label.tolist()\n",
        "    return data_X, data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTMeFWdU35Q5"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxUXy2va3B7c"
      },
      "source": [
        "def lr_model1(train_X, train_y):\n",
        "    model = LogisticRegression(random_state=0, C=20, fit_intercept=False) \n",
        "    model.fit(train_X, train_y)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL4JU5RznGYs"
      },
      "source": [
        "def lr_model2(train_X, train_y):\n",
        "    model = LogisticRegression(random_state=1, C=30, max_iter=200, fit_intercept=False)\n",
        "    model.fit(train_X, train_y)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W6Xa9SJ7LoZ"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OriIRGTF7NT8"
      },
      "source": [
        "def evaluation(model1, model2, test_X1, test_y1, test_y2):\n",
        "    pred_y1 = model1.predict(test_X1)\n",
        "    print(classification_report(test_y1, pred_y1, target_names=['NOT', 'OFF'], digits=2))\n",
        "    for i in range(len(pred_y1)):\n",
        "        if pred_y1[i] == 'OFF':\n",
        "            [pred_y1[i]] = model2.predict(test_X1[i])\n",
        "    target_names = ['ABU', 'NOT', 'OFF']\n",
        "    print(classification_report(test_y2, pred_y1, target_names=target_names, digits=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sood2vrR3-jP"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIbiSt1d38Tm",
        "outputId": "45a6e207-b538-4a26-93a3-6d54a3181711"
      },
      "source": [
        "print('###TRAIN Split###')\n",
        "train_data1, train_data2 = read_train_file('train_final_pp.csv')\n",
        "print('\\n###DEV Split###')\n",
        "dev_data1, dev_data2 = read_test_file('dev_final_pp.csv')\n",
        "print('\\n###TEST Split###')\n",
        "test_data1, test_data2 = read_test_file('test_final_pp.csv')\n",
        "train_X1, train_y1 = split_labels(train_data1)\n",
        "train_X2, train_y2 = split_labels(train_data2)\n",
        "dev_X1, dev_y1 = split_labels(dev_data1)\n",
        "dev_X2, dev_y2 = split_labels(dev_data2)\n",
        "test_X1, test_y1 = split_labels(test_data1)\n",
        "test_X2, test_y2 = split_labels(test_data2)\n",
        "\n",
        "all_tweets = train_X1 + dev_X1 + test_X1 \n",
        "vectorizer = TfidfVectorizer()\n",
        "#vectorizer = CountVectorizer()\n",
        "vectorizer.fit(all_tweets)\n",
        "train_vectors1 = vectorizer.transform(train_X1)\n",
        "dev_vectors1 = vectorizer.transform(dev_X1)\n",
        "test_vectors1 = vectorizer.transform(test_X1)\n",
        "train_vectors2 = vectorizer.transform(train_X2)\n",
        "\n",
        "model1 = lr_model1(train_vectors1, train_y1)\n",
        "model2 = lr_model2(train_vectors2, train_y2)\n",
        "print('\\n####DEV RESULTS####')\n",
        "evaluation(model1, model2, dev_vectors1, dev_y1, dev_y2)\n",
        "print('\\n####TEST RESULTS####')\n",
        "evaluation(model1, model2, test_vectors1, test_y1, test_y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###TRAIN Split###\n",
            "ABU: 1143\n",
            "OFF: 1445\n",
            "NOT: 5176\n",
            "\n",
            "###DEV Split###\n",
            "ABU: 110\n",
            "OFF: 76\n",
            "NOT: 361\n",
            "\n",
            "###TEST Split###\n",
            "ABU: 637\n",
            "OFF: 399\n",
            "NOT: 2072\n",
            "\n",
            "####DEV RESULTS####\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NOT      0.857     0.848     0.852       361\n",
            "         OFF      0.711     0.726     0.718       186\n",
            "\n",
            "    accuracy                          0.806       547\n",
            "   macro avg      0.784     0.787     0.785       547\n",
            "weighted avg      0.807     0.806     0.807       547\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ABU      0.621     0.536     0.576       110\n",
            "         NOT      0.857     0.848     0.852       361\n",
            "         OFF      0.400     0.500     0.444        76\n",
            "\n",
            "    accuracy                          0.737       547\n",
            "   macro avg      0.626     0.628     0.624       547\n",
            "weighted avg      0.746     0.737     0.740       547\n",
            "\n",
            "\n",
            "####TEST RESULTS####\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NOT      0.821     0.888     0.853      2072\n",
            "         OFF      0.733     0.614     0.668      1036\n",
            "\n",
            "    accuracy                          0.797      3108\n",
            "   macro avg      0.777     0.751     0.761      3108\n",
            "weighted avg      0.792     0.797     0.792      3108\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ABU      0.597     0.295     0.395       637\n",
            "         NOT      0.821     0.888     0.853      2072\n",
            "         OFF      0.268     0.371     0.311       399\n",
            "\n",
            "    accuracy                          0.700      3108\n",
            "   macro avg      0.562     0.518     0.520      3108\n",
            "weighted avg      0.704     0.700     0.690      3108\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}